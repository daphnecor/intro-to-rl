{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import poisson\n",
    "from typing import Tuple\n",
    "\n",
    "sns.set('notebook', font_scale=1.1, rc={'figure.figsize': (6, 6)})\n",
    "sns.set_style('ticks', rc={'figure.facecolor': 'none', 'axes.facecolor': 'none'})\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "matplotlib.rcParams['figure.facecolor'] = 'white'\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Policy Iteration: Jack's Car Rental\n",
    "\n",
    "---\n",
    "\n",
    "We implement **Exercise 4.7** and write a program for policy iteration to solve Jack's car rental problem exactly as shown in Figure 4.2 (See the policy iteration algorithm on page 80).\n",
    "\n",
    "$$\n",
    "$$\n",
    "\n",
    "**Intuition:** start with a guess policy --> evaluate --> improve it --> evaluate again --> improve it --> and so on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Parameters\n",
    "'''\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "LOCATIONS = [\"A\", \"B\"]\n",
    "\n",
    "# Pay for renting out a car \n",
    "CAR_REWARD = 10\n",
    "\n",
    "# Cars can be moved from one loc to the other\n",
    "CAR_MOVE_COST = 2\n",
    "\n",
    "# Assume that the number of cars requested and returned at each location are Poisson random variables\n",
    "LAMBDA_REQ = {\"A\": 3, \"B\": 4}\n",
    "LAMBDA_RETURN = {\"A\": 4, \"B\": 2}\n",
    "\n",
    "# Assume that there can be no more than 20 cars per location\n",
    "MAX_CARS = 5\n",
    "\n",
    "# The maximum cars that can be moved overnight\n",
    "MAX_MOVING_CARS = 3\n",
    "\n",
    "# Discount rate\n",
    "GAMMA = 0.9\n",
    "\n",
    "# The action indicates how many cars to move from point A to B or B to A\n",
    "# Positive: move cars from A --> B | Negative: move cars from B --> A\n",
    "ACTIONS = np.arange(-MAX_MOVING_CARS, MAX_MOVING_CARS + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "\n",
    "MAX_CARS_POISSON = 11\n",
    "\n",
    "@lru_cache\n",
    "def prob_cars(cars: int, lambda_: int) -> float:\n",
    "    return poisson.pmf(k=cars, mu=lambda_)\n",
    "\n",
    "@lru_cache\n",
    "def prob_state_reward_given_state_action(\n",
    "    state_prime: Tuple[int, int],\n",
    "    reward: int,\n",
    "    state: Tuple[int, int],\n",
    "    action: int,\n",
    ") -> float:\n",
    "    net_cars_a = (state[0] + max(0, action)) - state_prime[0]\n",
    "    net_cars_b = (state[1] - min(0, action)) - state_prime[1]\n",
    "    return np.sum(\n",
    "        [\n",
    "            (\n",
    "                prob_cars(req_cars_a, LAMBDA_REQ[\"A\"]) *\n",
    "                prob_cars(return_cars_a, LAMBDA_RETURN[\"A\"])\n",
    "            ) * (\n",
    "                prob_cars(req_cars_b, LAMBDA_REQ[\"B\"]) *\n",
    "                prob_cars(return_cars_b, LAMBDA_RETURN[\"B\"])\n",
    "            )\n",
    "            for req_cars_a in range(MAX_CARS_POISSON + 1)\n",
    "            for return_cars_a in range(MAX_CARS_POISSON + 1)\n",
    "            for req_cars_b in range(MAX_CARS_POISSON + 1)\n",
    "            for return_cars_b in range(MAX_CARS_POISSON + 1)\n",
    "            if (\n",
    "                req_cars_a + return_cars_a == net_cars_a\n",
    "            ) and (\n",
    "                req_cars_b + return_cars_b == net_cars_b\n",
    "            ) and (\n",
    "                reward == (\n",
    "                    (req_cars_a + req_cars_b) * CAR_REWARD\n",
    "                        - np.abs(action) * CAR_MOVE_COST\n",
    "                )\n",
    "            )  \n",
    "        ]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ITER_POLICY_EVALUATION = 5\n",
    "MAX_ITER_POLICY_IMPROVEMENT = 5\n",
    "\n",
    "states = [\n",
    "    (cars_a, cars_b)\n",
    "    for cars_a in range(0, MAX_CARS + 1)\n",
    "    for cars_b in range(0, MAX_CARS + 1)\n",
    "]\n",
    "\n",
    "rewards = np.sort(\n",
    "    np.unique(\n",
    "        [\n",
    "            (\n",
    "                (rent_cars_a + rent_cars_b) * CAR_REWARD\n",
    "                    - np.abs(move_cars) * -CAR_MOVE_COST\n",
    "            )\n",
    "            for rent_cars_a in range(MAX_CARS + 1)\n",
    "            for rent_cars_b in range(MAX_CARS + 1)\n",
    "            for move_cars in ACTIONS\n",
    "        ],\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# Step 1. Initialization\n",
    "values = np.zeros(shape=(MAX_CARS + 1, MAX_CARS + 1), dtype=int)\n",
    "policies = np.zeros(shape=(MAX_CARS + 1, MAX_CARS + 1), dtype=int)\n",
    "\n",
    "policy_improvement_iters = 0\n",
    "policy_stable = False\n",
    "while not policy_stable and policy_improvement_iters < MAX_ITER_POLICY_IMPROVEMENT:\n",
    "    policy_improvement_iters += 1\n",
    "\n",
    "    delta = np.inf\n",
    "    policy_evaluation_iters = 0\n",
    "    # Step 2. Policy Evaluation\n",
    "    while delta > THETA and policy_evaluation_iters < MAX_ITER_POLICY_EVALUATION:\n",
    "        policy_evaluation_iters += 1\n",
    "        delta = 0\n",
    "        for state in states:\n",
    "            state_value = values[state]\n",
    "            action = policies[state]\n",
    "            values[state] = np.sum(\n",
    "                [\n",
    "                    prob_state_reward_given_state_action(\n",
    "                        state_prime=state_prime,\n",
    "                        reward=reward,\n",
    "                        state=state,\n",
    "                        action=action,\n",
    "                    ) * (reward + GAMMA * values[state_prime])\n",
    "                    for state_prime in states\n",
    "                    for reward in rewards\n",
    "                ]\n",
    "            )\n",
    "            delta = max(delta, np.abs(state_value - values[state]))\n",
    "        \n",
    "        # _, ax = plt.subplots()\n",
    "        # sns.heatmap(values, cbar=True, cmap='Blues', ax=ax)\n",
    "        # ax.set_title(f'k = {policy_evaluation_iters}', loc='left')\n",
    "        # ax.set_title(r'Policy evaluation | $v_{\\pi}k$', y=1.05)\n",
    "        # ax.set_xlabel('Number of cars at location A')\n",
    "        # ax.set_ylabel('Number of cars at location B')\n",
    "        # plt.show()\n",
    "        \n",
    "        logging.info(f\"Completed Policy Evaluation {policy_evaluation_iters}\")\n",
    "\n",
    "        if delta < THETA:\n",
    "            break\n",
    "\n",
    "    # Step 3. Policy Improvement\n",
    "    policy_stable = True\n",
    "    for state in states:\n",
    "        old_action = policies[state]\n",
    "        policies[state] = np.argmax(\n",
    "            [\n",
    "                np.sum(\n",
    "                    [\n",
    "                        prob_state_reward_given_state_action(\n",
    "                            state_prime=state_prime,\n",
    "                            reward=reward,\n",
    "                            state=state,\n",
    "                            action=action,\n",
    "                        ) * (reward + GAMMA * values[state_prime])\n",
    "                        for reward in rewards\n",
    "                        for state_prime in states\n",
    "                    ]\n",
    "                )\n",
    "                for action in ACTIONS\n",
    "            ]\n",
    "        ) - MAX_MOVING_CARS\n",
    "\n",
    "        if old_action != policies[state]:\n",
    "            policy_stable = False\n",
    "\n",
    "    _, ax = plt.subplots()\n",
    "    sns.heatmap(policies, cbar=True, cmap='Blues', ax=ax)\n",
    "    ax.set_title(f'k = {policy_improvement_iters}', loc='left')\n",
    "    ax.set_title(r'Policy Improvement | $\\pi_k$', y=1.05)\n",
    "    ax.set_xlabel('Number of cars at location A')\n",
    "    ax.set_ylabel('Number of cars at location B')\n",
    "    plt.show()\n",
    "\n",
    "    logging.info(f\"Completed Policy Improvement {policy_improvement_iters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(MAX_CARS+1)\n",
    "y = np.arange(MAX_CARS+1)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.suptitle(f'v Ï€ k = {policy_evaluation_iters}',)\n",
    "ax = plt.axes(projection='3d')\n",
    "im = ax.plot_surface(X, Y, values, rstride=1, cstride=1, cmap='viridis', edgecolor='none')\n",
    "ax.set_zlabel('Return')\n",
    "ax.set_xlabel('# of cars at loc A')\n",
    "ax.set_ylabel('# of cars at loc B')\n",
    "ax.view_init(30, 150)\n",
    "plt.colorbar(im);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9 (default, Oct 26 2021, 07:25:53) \n[Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
