{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import poisson\n",
    "from typing import Tuple\n",
    "\n",
    "sns.set('notebook', font_scale=1.1, rc={'figure.figsize': (8, 4)})\n",
    "sns.set_style('ticks', rc={'figure.facecolor': 'none', 'axes.facecolor': 'none'})\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "matplotlib.rcParams['figure.facecolor'] = 'white'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Sarsa\n",
    "\n",
    "Implement Sarsa($\\lambda$) in Easy21. Initialize the value function to zero. Use the same step-size and exploration schedules as in the previous section. Run the algorithm with parameter values $\\lambda \\in \\{ 0, 0.1, 0.2, \\dots, 1\\}$. Stop each run after 1000 episodes and report the mean-squared error (MSE):\n",
    "\\begin{align}\n",
    "    \\sum_{s, a} (Q(s, a) - Q^{*}(s, a))^2\n",
    "\\end{align}\n",
    "over all states $s$ and actions $a$, comparing the true values $Q^{*}(s, a)$ computed in the previous section with the estimated values $Q(s, a)$ computed by Sarsa. Plot the mean-squared error against $\\lambda$. For $\\lambda=0$ and $\\lambda=1$ only, plot the learning curve of the MSE against the episode number.\n",
    "\n",
    "---\n",
    "\n",
    "We have the following pseudocode:\n",
    "\n",
    "```python\n",
    "for episode in range(num_episodes):\n",
    "\n",
    "    initialize state matrix\n",
    "\n",
    "    while game is not done:\n",
    "\n",
    "        take action, observe reward and new_state\n",
    "\n",
    "        choose new_action using fixed policy\n",
    "\n",
    "        # Update rule\n",
    "        Q[s, a] += Q[s, a] + alpha * (reward + gamma * (Q[new_state, new_action] - Q[s, a]))\n",
    "\n",
    "        # Update current state, action\n",
    "        state = new_state\n",
    "        action = new_action\n",
    "```\n",
    "\n",
    "We implement SARSA, which has the following update rule (Sutton & Barto, section 6.4, page 129):\n",
    "\n",
    "\\begin{align}\n",
    "    Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\left[ R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \\right]\n",
    "\\end{align}\n",
    "\n",
    "**Note that**\n",
    "- We do this update for each transition to a nonterminal state (in contrast to MC methods)\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSA(BaseAlgo):\n",
    "    \"\"\"TD Learning for policy evaluation.\n",
    "\n",
    "    Args:\n",
    "        BaseAlgo (object): Base RL algorithm class.\n",
    "    \"\"\"\n",
    "    def __init__(self, env_dim, N0, lambda_coef, gamma):\n",
    "        super(SARSA, self).__init__(env_dim, N0)\n",
    "        self.gamma = gamma\n",
    "        self.lambda_coef = lambda_coef\n",
    "        self.Q_star = None\n",
    "        self.mse_episode = None\n",
    "\n",
    "    def update_policy(self, obs, action, new_obs, new_action, reward):\n",
    "        \"\"\"Method to update the policy.\n",
    "\n",
    "        Args:\n",
    "            obs (int): The state\n",
    "            action, new_action (int): Whether to hit or stick\n",
    "            new_obs (_type_): The new state\n",
    "            reward (_type_): Obtained reward for each state\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.N_table[self.to_index(obs, action)] != 0:\n",
    "            alpha = 1 / self.N_table[self.to_index(obs, action)]\n",
    "        else: \n",
    "            alpha = 1\n",
    "        \n",
    "        # Compute td error (sometimes called delta)      \n",
    "        td_error = (\n",
    "            reward \n",
    "          + self.gamma\n",
    "          * self.Q_table[self.to_index(new_obs, new_action)] \n",
    "          - self.Q_table[self.to_index(obs, action)]\n",
    "        )\n",
    "\n",
    "        # Update Q value\n",
    "        self.Q_table[self.to_index(obs, action)] += alpha * td_error\n",
    "\n",
    "    def compute_mse(self, Q_star, episode):\n",
    "        \"\"\"Compute mean squared error (MSE) to ground-truth.\n",
    "\n",
    "        Args:\n",
    "            Q_star (np.array): Ground-truth obtained through MC learning with many episodes\n",
    "            episode (int): The episode in which we are.\n",
    "        \"\"\"\n",
    "        self.mse_episode[episode] = np.sum(\n",
    "            np.square(self.Q - Q_star)) / float(Q_star.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 1000\n",
    "\n",
    "# Initialize environment\n",
    "easy21 = Easy21(dealer_threshold=17, player_threshold=12)\n",
    "\n",
    "# Initialize Monte Carlo algorithm\n",
    "sarsa_learner = SARSA(env_dim=easy21.dim, gamma=.9, lambda_coef=1, N0=100)\n",
    "\n",
    "for episode_i in range(num_episodes):\n",
    "    \n",
    "    #print(f\"episode: {episode_i}\")\n",
    "    obs, _ , done, info = easy21._reset()\n",
    "\n",
    "    while not done:\n",
    "        if obs[1] < easy21.player_threshold: # Play with fixed policy\n",
    "            # Always hit\n",
    "            action = 1 \n",
    "        else:  \n",
    "            # Take epsilon greedy action\n",
    "            action = monte_carlo_learner.take_eps_greedy_action(obs)\n",
    "            \n",
    "        # Get new observation\n",
    "        new_obs, reward, done, info = easy21.step(action)\n",
    "        obs = new_obs\n",
    "    \n",
    "        # Update the policy at the end of the episode\n",
    "        sarsa_learner.update_policy(obs, action, new_obs, action, reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_td_lambda(lambda_coef, Q_star, num_episodes=1000, N0=100, gamma=.9):\n",
    "    \n",
    "    # Initialize environment\n",
    "    easy21 = Easy21(\n",
    "        dealer_threshold=17, \n",
    "        player_threshold=12,\n",
    "    )\n",
    "\n",
    "    # Initialize SARSA(lambda) learner\n",
    "    sarsa_learner = SARSA(\n",
    "        env_dim=easy21.dim, \n",
    "        gamma=gamma, \n",
    "        lambda_coef=lambda_coef, \n",
    "        N0=N0,\n",
    "    )\n",
    "\n",
    "    for episode_i in range(num_episodes):\n",
    "        \n",
    "        # Initial game state\n",
    "        obs, _ , done, info = easy21._reset() \n",
    "        action = sarsa_learner.take_eps_greedy_action(obs)\n",
    "\n",
    "        while not done:\n",
    "            if obs[1] < easy21.player_threshold: # Play with fixed policy\n",
    "                # Always hit\n",
    "                action = 1 \n",
    "            else:  \n",
    "                # Take epsilon greedy action\n",
    "                action = monte_carlo_learner.take_eps_greedy_action(obs)\n",
    "                \n",
    "            # Get new observation\n",
    "            new_obs, reward, done, info = easy21.step(action)\n",
    "            \n",
    "            # Get new action        \n",
    "            new_action = sarsa_learner.take_eps_greedy_action()\n",
    "\n",
    "            # Update the policy\n",
    "            sarsa_learner.update_policy(obs, action, new_obs, new_action, reward)\n",
    "\n",
    "            obs = new_obs\n",
    "            action = new_action\n",
    "\n",
    "        sarsa_learner.compute_mse(Q_star, episode_i)\n",
    "\n",
    "    return sarsa_learner.mse_episode"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
